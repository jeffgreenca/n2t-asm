package main

import (
	"strings"
	"testing"

	"github.com/jeffgreenca/n2t-asm/internal/pkg/assembler"
	"github.com/jeffgreenca/n2t-asm/internal/pkg/lex"
	"github.com/jeffgreenca/n2t-asm/internal/pkg/parser"
	"github.com/stretchr/testify/assert"
)

const (
	progTrivial = `
		// comment
		@5
		D=A
		@10
		M=D+1
`
	// generated by hand via nand2tetris provided assembler
	progTrivialExpected = `0000000000000101
1110110000010000
0000000000001010
1110011111001000`

	progAdd = `
	// Computes R0 = 2 + 3  (R0 refers to RAM[0])
		@2
		D=A
		@3
		D=D+A
		@0
		M=D
	(END)
		@END
		0;JMP
`

	// generated by hand via nand2tetris provided assembler
	progAddExpected = `0000000000000010
1110110000010000
0000000000000011
1110000010010000
0000000000000000
1110001100001000
0000000000000110
1110101010000111`
)

func tokenize(s string) ([]lex.Token, error) {
	// legacy bridge
	return lex.Tokenize(strings.NewReader(s))
}

func TestLexParseSingle(t *testing.T) {
	tokens, err := tokenize("@10")
	assert.NoError(t, err)
	assert.Len(t, tokens, 3)

	p := parser.New()
	prog, err := p.Parse(tokens)
	assert.NoError(t, err)
	assert.Len(t, prog, 1)
}

func TestLexParseAssembleNoLabels(t *testing.T) {
	var tokens []lex.Token
	for _, s := range strings.Split(progTrivial, "\n") {
		tk, err := tokenize(s)
		assert.NoError(t, err)
		tokens = append(tokens, tk...)
	}

	p := parser.New()
	prog, err := p.Parse(tokens)
	assert.NoError(t, err)
	assert.Len(t, prog, 4)

	hack, err := assembler.Assemble(prog)
	assert.NoError(t, err)
	assert.Equal(t, progTrivialExpected, strings.Join(hack, "\n"))
}

func TestAll(t *testing.T) {
	var tokens []lex.Token
	for _, s := range strings.Split(progAdd, "\n") {
		tk, err := tokenize(s)
		assert.NoError(t, err)
		tokens = append(tokens, tk...)
	}

	p := parser.New()
	prog, err := p.Parse(tokens)
	assert.NoError(t, err)
	assert.Len(t, prog, 9)

	hack, err := assembler.Assemble(prog)
	assert.NoError(t, err)
	assert.Equal(t, progAddExpected, strings.Join(hack, "\n"))
}

// --- regression tests
func TestLexParseDMJGT(t *testing.T) {
	// D;JGT -> 1110001100000001
	tokens, err := tokenize("D;JGT")
	assert.NoError(t, err)
	assert.Len(t, tokens, 3)
	assert.Equal(t, lex.Token{Value: "D", Type: lex.LOCATION}, tokens[0])
	assert.Equal(t, lex.Token{Value: "JGT", Type: lex.JUMP}, tokens[1])
	assert.Equal(t, lex.Token{Type: lex.END}, tokens[2])

	p := parser.New()
	prog, err := p.Parse(tokens)
	assert.NoError(t, err)
	assert.Len(t, prog, 1)
	assert.Equal(t, parser.Command{Type: parser.C_COMMAND, C: parser.CmdC{D: parser.Dest{}, C: "D", J: "JGT"}}, prog[0])

	hack, err := assembler.Assemble(prog)
	assert.NoError(t, err)
	assert.Equal(t, "1110001100000001", hack[0])
}
